{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac192746-33f5-40cf-a9dd-4b94e81824c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jax import jit\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "# from jax.scipy.linalg import solve\n",
    "\n",
    "import Surprise_statistics as sup # import surprise code\n",
    "import PPD # import ppd code\n",
    "import surprise_gaussian as supg # gaussian surprise code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4d7f0-6aa9-4a07-85e3-7a0de8f6edb3",
   "metadata": {},
   "source": [
    "Para calcular a divergência de Kullback-Leibler (KLD) entre duas distribuições $p_2$ e $p_1$, $D(p_2|p_1)$, sua distribuição e consequentemente seu valor esperado precisamos efetivamente nos importarmos com duas distribuições (i.e., se considerarmos uma priori plana, infinita e não informativa), $p(\\theta|D_1)$, a posteriori do observável 1 e a likelihood do observável 2, i.e., $\\mathcal{L}(D_2|\\Theta)$.\n",
    "\n",
    "Como estamos interessados em encontrar a Surpresa entre as duas distribuições, em um outro passo vamos precisar de calcular a posteriori $p(\\theta|D_2)$.\n",
    "\n",
    "Vamos considerar o seguinte caso, um vetor de dados $D_i = F(\\theta)$ onde dim($\\theta$) = 2 e dim(D$_i$) = 7. $F(\\theta)$ é uma função linear que mapeia os parâmetros $\\theta$ nos dados $D_i$. Considere as seguintes informações, onde M é uma matriz 7x1:\n",
    "\n",
    "\\begin{align}\n",
    "    p(\\theta|D_1) &= \\mathcal{N}(\\theta_1, \\Sigma_1)\\\\\n",
    "    \\mathcal{L}(D_2|\\Theta) &= \\mathcal{N}(F(\\theta), \\mathcal{C})\\\\\n",
    "    F(\\theta) &= F_0+M\\theta \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f610eac8-e305-4f04-995b-195e09300c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiducial parameters 1 =  [0.4 0.1]\n",
      "Fiducial parameters 2 =  [1.3 0.2]\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants and dimensions\n",
    "theta_dim = 2\n",
    "D_dim = 7\n",
    "\n",
    "# Gera um vetor de parametros \\theta_1 e theta_2\n",
    "theta_fid_1 = np.array([0.4, 0.1])\n",
    "theta_fid_2 = np.array([1.3, 0.2])\n",
    "\n",
    "\n",
    "# generate F(theta) related quantities\n",
    "F0 = np.random.rand(D_dim)  # 7x1 vector\n",
    "M = np.random.rand(D_dim, theta_dim)  # 7x2 matrix\n",
    "# generate covariance matrix for likelihood L2\n",
    "C = np.random.rand(D_dim, D_dim)  # 7x7 matrix\n",
    "# generate covariance matrix for posterior 1\n",
    "Sigma_1 = np.random.rand(theta_dim, theta_dim)  # 2x2 matrix\n",
    "\n",
    "# Ensure C and Sigma are symmetric and positive-definite\n",
    "C = np.dot(C, C.T)\n",
    "Sigma_1 = np.dot(Sigma_1, Sigma_1.T)\n",
    "invS1 = np.linalg.inv(Sigma_1)\n",
    "\n",
    "# covariance matrix of posterior 2\n",
    "# Assuming Gaussianity and a flat prior, we can also derive the equations for Sigma_2. \n",
    "# See https://arxiv.org/abs/1402.3593 eq. A17.\n",
    "invC  = np.linalg.inv(C)\n",
    "invS2 = np.dot(M.T, np.dot(invC, M))\n",
    "Sigma_2 = np.linalg.inv(invS2)\n",
    "\n",
    "# Define the linear function F(theta)\n",
    "def F(theta):\n",
    "    return F0 + np.dot(M, theta)\n",
    "\n",
    "# fiducial data vectors\n",
    "################# hey there, maybe you should think some more here about putting noise to this vector!\n",
    "D1_fid = F(theta_fid_1) \n",
    "D2_fid = F(theta_fid_2)\n",
    "\n",
    "print(\"Fiducial parameters 1 = \", theta_fid_1)\n",
    "print(\"Fiducial parameters 2 = \", theta_fid_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89d177-1c37-4b54-8762-706725720102",
   "metadata": {},
   "source": [
    "**Define a multivariate gaussian distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb980bc-c88b-4841-9269-158dd6731561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def multivariate_gaussian_pdf(theta, mean, cov):\n",
    "    \"\"\"\n",
    "    Calculate the PDF of a multivariate Gaussian distribution.\n",
    "\n",
    "    Parameters:\n",
    "    mean : array-like, shape (n,)\n",
    "        The mean vector of the Gaussian distribution.\n",
    "    cov : array-like, shape (n, n)\n",
    "        The covariance matrix of the Gaussian distribution.\n",
    "    theta : array-like, shape (n,)\n",
    "        The parameter vector at which to evaluate the PDF.\n",
    "\n",
    "    Returns:\n",
    "    pdf_value : float\n",
    "        The PDF value of the multivariate Gaussian at the data point.\n",
    "    \"\"\"\n",
    "    k = mean.shape[0]\n",
    "    diff = theta - mean\n",
    "    inv_cov = jnp.linalg.inv(cov)\n",
    "    logL = -0.5 * jnp.dot(diff, jnp.dot(inv_cov, diff))\n",
    "    norm_factor = jnp.log(jnp.sqrt((2 * jnp.pi) ** k * jnp.linalg.det(cov)))\n",
    "    logpdf_value = logL - norm_factor\n",
    "    return logpdf_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a1339-a1a1-4472-8929-a235cf0238b3",
   "metadata": {},
   "source": [
    "**Define both loglikelihoods that will be used in the main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f44522-f356-48f0-9955-70d2d40bd132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the distributions as callable functions of theta and D\n",
    "@jit\n",
    "def logL1(theta):\n",
    "    return multivariate_gaussian_pdf(theta, theta_fid_1, Sigma_1)\n",
    "    \n",
    "def logL2(theta, D2):\n",
    "    return multivariate_gaussian_pdf(theta, theta_fid_2, Sigma_2)\n",
    "\n",
    "domain = np.array([[5,-5], [5,-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e979f0b-f8ea-4d03-ba2d-c50c9ba51f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3221e707-cc13-4405-ad59-aad67144fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Loading posterior data\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Loading failed!\n",
      "----------------------------------------------------------------------\n",
      "Running Nested Sampling...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/prm/anaconda3/envs/surprise/lib/python3.10/site-packages/dynesty/dynamicsampler.py:453: RuntimeWarning: overflow encountered in cast\n",
      "  cur_live_logl[not_finite] = _LOWL_VAL\n",
      "18122it [00:13, 1355.57it/s, batch: 18 | bound: 3 | nc: 1 | ncall: 89893 | eff(%): 20.021 | loglstar: -8.106 < -1.374 < -1.857 | logz: -4.609 +/-  0.038 | stop:  0.970]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed sucessfully.\n",
      "Saving  None\n",
      "Error while saving\n",
      "Done!\n",
      "Evaluating theory from sample distribution p1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edf3836c059464bb1e6b91b2f4b8724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating theory vectors:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling the Posterior Predictive Distribution...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d4b1466e3247e68c2b04cfff292f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling PPD:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82023419af39464485641b63beb334df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over the PPD:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/prm/anaconda3/envs/surprise/lib/python3.10/site-packages/dynesty/dynamicsampler.py:453: RuntimeWarning: overflow encountered in cast\n",
      "  cur_live_logl[not_finite] = _LOWL_VAL\n",
      "25025it [01:11, 351.40it/s, batch: 26 | bound: 4 | nc: 1 | ncall: 164027 | eff(%): 15.113 | loglstar: -8.452 < -0.337 < -0.802 | logz: -4.631 +/-  0.040 | stop:  0.971]          \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c965088643a471885239ea50d66e132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3b257428224317a2a83f20a632974d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value = 20.0 %\n",
      "S = 0.01 nats\n",
      "<KLD> = 2.21 nats\n",
      "KLD = 2.22 nats\n",
      "Saving results\n",
      "----------------------------------------------------------------------\n",
      "Failed saving results\n"
     ]
    }
   ],
   "source": [
    "surprise_results = surprise_function_call(logL1, logL2, data2_model_fun=F, covariance_matrix_2=C, domain=domain, Nkld = 5, result_path=\"DELETAR.hdf5\", n_jobs=5, data_2_vec = D2_fid)\n",
    "\n",
    "# surprise_results = surprise_function_call(logL1, logL2, data2_model_fun=F, covariance_matrix_2=C, domain=domain, Nkld=5, result_path=\".\", data_1_name="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c66676fd-8998-4310-a1f3-15e3dab30909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_hdf5(file_name, data_dict):\n",
    "    \"\"\"\n",
    "    Save the contents of a dictionary to an HDF5 file, handling different data types.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name: The name of the HDF5 file to be created.\n",
    "    - data_dict: The dictionary to save, where keys are dataset names and values are the data.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_name, 'w') as hdf:\n",
    "        for key, value in data_dict.items():\n",
    "            # Check the type of the value to handle it appropriately\n",
    "            if isinstance(value, np.ndarray):\n",
    "                # Save NumPy arrays directly as datasets\n",
    "                hdf.create_dataset(key, data=value)\n",
    "            elif isinstance(value, jnp.ndarray):\n",
    "                # Convert JAX array to NumPy array and save it\n",
    "                hdf.create_dataset(key, data=np.array(value))\n",
    "            elif isinstance(value, (float, int, np.float32, np.float64, np.int32, np.int64)):\n",
    "                # Save floats or integers as attributes of a dataset\n",
    "                # If the value is a NumPy scalar, convert to a native Python type\n",
    "                if hasattr(value, 'item'):\n",
    "                    value = value.item()\n",
    "                dset = hdf.create_dataset(key, data=[])\n",
    "                dset.attrs['value'] = value\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported data type for key '{key}': {type(value)}\")\n",
    "                \n",
    "save_dict_to_hdf5(\"TESTARDELETAR.hdf5\", surprise_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3ca08f9-2234-44db-895d-9530f89ffe30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': Array(-0.08835649, dtype=float32),\n",
       " 'S_dist': array([ 0.02267694, -0.00136876,  0.03353047, -0.03103805, -0.02380133],\n",
       "       dtype=float32),\n",
       " 'kld21': Array(2.0961416, dtype=float32),\n",
       " 'kld_exp': 2.184498,\n",
       " 'kld_dist': array([2.207175 , 2.1831293, 2.2180285, 2.15346  , 2.1606967],\n",
       "       dtype=float32),\n",
       " 'p_value': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a67bb0-2b4f-45bd-970e-a5a1fb72a0e5",
   "metadata": {},
   "source": [
    "# Funções para add no código final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b3adfbc-8ee5-4943-bd6e-ae16a8c72e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pval(Sdist, S, verbose = 0):\n",
    "    \"\"\"\n",
    "    Calculate the p-value from a distribution of surprise values given an observed surprise.\n",
    "\n",
    "    Parameters:\n",
    "    - Sdist (ndarray): An array of surprise values from simulations or a distribution.\n",
    "    - S (float): The observed surprise value for which the p-value is to be calculated.\n",
    "\n",
    "    Returns:\n",
    "    - pval (float): The calculated p-value indicating the probability of observing a surprise at least as extreme as S.\n",
    "    \"\"\"\n",
    "    pval = Sdist[Sdist > S].size/Sdist.size\n",
    "    if verbose>0:\n",
    "        print(\"p-value = {:.1f} %\".format(100*pval))\n",
    "    return pval\n",
    "\n",
    "from scipy import special\n",
    "def sigma_discordance(p_value):\n",
    "    return np.sqrt(2)*special.erfinv(1-p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32b94f46-d3b6-4e9c-aaac-1d43b7ff3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save_dict_to_hdf5(file_name, data_dict):\n",
    "    \"\"\"\n",
    "    Save the contents of a dictionary to an HDF5 file, handling different data types.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name: The name of the HDF5 file to be created.\n",
    "    - data_dict: The dictionary to save, where keys are dataset names and values are the data.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_name, 'w') as hdf:\n",
    "        for key, value in data_dict.items():\n",
    "            # Check the type of the value to handle it appropriately\n",
    "            if isinstance(value, np.ndarray):\n",
    "                # Save NumPy arrays directly as datasets\n",
    "                hdf.create_dataset(key, data=value)\n",
    "            elif isinstance(value, jnp.ndarray):\n",
    "                # Convert JAX array to NumPy array and save it\n",
    "                hdf.create_dataset(key, data=np.array(value))\n",
    "            elif isinstance(value, (float, int, np.float32, np.float64, np.int32, np.int64)):\n",
    "                # Save floats or integers as attributes of a dataset\n",
    "                # If the value is a NumPy scalar, convert to a native Python type\n",
    "                if hasattr(value, 'item'):\n",
    "                    value = value.item()\n",
    "                dset = hdf.create_dataset(key, data=[])\n",
    "                dset.attrs['value'] = value\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported data type for key '{key}': {type(value)}\")\n",
    "            \n",
    "def load_create_NS_file(data_1_name, logL1, ndim, domain,  n_effective=15000, dlogz=0.5):\n",
    "    print(70*'-')\n",
    "    print(\"Loading posterior data\")\n",
    "    print(70*'-')\n",
    "    try: \n",
    "        ## loading pre-made Nested Sampling run\n",
    "        res_1 = joblib.load(data_1_name)\n",
    "        print(\"Data loaded sucessfully!\")\n",
    "    except:\n",
    "        print(70*'-')\n",
    "        print(\"Loading failed!\")\n",
    "        print(70*'-')\n",
    "\n",
    "        print(\"Running Nested Sampling...\")\n",
    "        print(70*'-')\n",
    "        # any nested sampling routine will be fine, as long as it has the method .samples_equal() or equivalent, to obtain equally weighted samples. \n",
    "        res_1 = sup.run_nested_sampling(logL1, ndim, domain=domain, print_progress=True, n_effective=n_effective, dlogz=dlogz)\n",
    "        print(\"Run completed sucessfully.\")\n",
    "        \n",
    "        # if data_1_name is not None:\n",
    "        try:\n",
    "            print(\"Saving \", data_1_name)\n",
    "            joblib.dump(res_1, data_1_name)\n",
    "        except:\n",
    "            # print(\"Error while saving\")\n",
    "            pass\n",
    "    return res_1\n",
    "    \n",
    "def surprise_function_call(logL1, logL2, data2_model_fun, covariance_matrix_2, domain, Nkld, \n",
    "                           result_path, data_1_name = None, n_effective= 15000, n_jobs=-1, data_2_vec = None, data_2_name = None, verbose=1):\n",
    "    # logL1 --> a callable function of theta (parameter)\n",
    "    # logL2 --> a callable function of theta (parameter) and D (data).\n",
    "    # data_2_vec is yet to be added. If provided then function should also compute KLD(p2|p1) \n",
    "    # and return the surprise statistic value  \n",
    "    ndim = domain.shape[0]\n",
    "\n",
    "    if verbose>0:\n",
    "        print_progress = True\n",
    "    else:\n",
    "        print_progress = False\n",
    "\n",
    "    ############ loading/creating mock 1 ############\n",
    "    res_1 = load_create_NS_file(data_1_name, logL1, ndim, domain)\n",
    "    \n",
    "    # This method is a particularity of Dynesty, but can be easily implemented for any other NS package.\n",
    "    # Equal weighted samples \n",
    "    eq_samples_1 = res_1.samples_equal()\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    ############ create posterior predictive distribution ############\n",
    "    # parameter space samples of posterior distribution p(theta|D1)\n",
    "    th1_samples = sup.sampler(eq_samples_1, Nkld) # we take a subset of samples with size Nkld\n",
    "    PPD_chain = PPD.create_ppd_chain(th1_samples=th1_samples, data_model_fun=data2_model_fun, cov_matrix = covariance_matrix_2, sample_size=1, n_jobs=n_jobs)\n",
    "\n",
    "    kld_samples = sup.main_parallel(PPD_chain, logL2, res_1, logP_1=logL1,  domain=domain, mode='MCMC', n_jobs=n_jobs, \n",
    "              result_path=None, ignore_warnings=True, \n",
    "              n_effective=n_effective)\n",
    "\n",
    "    kld_array = np.array(kld_samples)\n",
    "    kld_exp = kld_array.mean()\n",
    "    S_dist = kld_array - kld_exp\n",
    "\n",
    "    # if data 2 is provided\n",
    "    if data_2_vec is not None:\n",
    "        logP2 = lambda theta : logL2(theta, data_2_vec)\n",
    "\n",
    "        # load or create data_2 posterior chain with NS\n",
    "        if data_2_name is not None:\n",
    "            res_2 = load_create_NS_file(data_2_name, logP2, ndim, domain)\n",
    "        else:\n",
    "            res_2 = sup.run_nested_sampling(logP2, ndim, domain=domain, print_progress=True)\n",
    "        \n",
    "        kld_value = sup.compute_KLD_MCMC(res_2, logP2, res_1, logL1, domain = domain)\n",
    "        S = kld_value - kld_exp\n",
    "        p_value = find_pval(S_dist, S, verbose = verbose)\n",
    "        if verbose>0:\n",
    "            print(\"S = {:.2f} nats\".format(S))\n",
    "            print(\"<KLD> = {:.2f} nats\".format(kld_exp))\n",
    "            print(\"KLD = {:.2f} nats\".format(kld_value))\n",
    "        results_dic = {\"S\" : S, \"S_dist\": S_dist, \"kld21\" : kld_value, \"kld_exp\":kld_exp, \"kld_dist\":kld_array, \"p_value\":p_value}\n",
    "    else:\n",
    "        if verbose>0:\n",
    "            print(\"<KLD> = {:.2f} nats\".format(kld_exp))\n",
    "        results_dic = {\"S_dist\": S_dist, \"kld_exp\":kld_exp, \"kld_dist\":kld_array}\n",
    "    if result_path is not None:\n",
    "        try:\n",
    "            print(\"Saving results\")\n",
    "            print(70*\"-\")\n",
    "            save_dict_to_hdf5(result_path, results_dic)\n",
    "            print(\"Results saved sucessfully!\")\n",
    "        except:\n",
    "            print(\"Failed saving results\")\n",
    "            pass\n",
    "    return results_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6dcf20-05d7-4d42-8717-a3b1f6571134",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa98611-8dec-4e63-bd99-48f5144c9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f4d2b4-f50c-4b5f-b344-51b0ad19768f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90068067, 0.52135128, 0.90928984],\n",
       "       [0.69997009, 0.59434248, 0.04956948],\n",
       "       [0.0891736 , 0.8285129 , 0.59819861],\n",
       "       [0.53530764, 0.24321398, 0.18703713],\n",
       "       [0.61414684, 0.10966708, 0.20365686],\n",
       "       [0.54760663, 0.48011745, 0.86623483],\n",
       "       [0.13129248, 0.37410539, 0.43129792],\n",
       "       [0.69791098, 0.15303223, 0.93526071],\n",
       "       [0.65757065, 0.55555849, 0.35832369],\n",
       "       [0.84353273, 0.20610911, 0.21355773]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for the uniform distribution of wb\n",
    "# Calculate the range for the uniform distribution\n",
    "lower_bound = DESI.mu_wb - 5 * DESI.sigma_wb\n",
    "upper_bound = DESI.mu_wb + 5 * DESI.sigma_wb\n",
    "\n",
    "# Generate uniform samples for the new dimension wb\n",
    "wb_values = np.random.uniform(lower_bound, upper_bound, size=(eq_samples_1.shape[0], 1))\n",
    "\n",
    "# Concatenate the new dimension to the existing samples\n",
    "eq_samples_with_wb = np.concatenate((eq_samples_1, wb_values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990cf21-7363-4395-9315-95a83af40e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (surprise)",
   "language": "python",
   "name": "surprise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
